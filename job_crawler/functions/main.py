import concurrent.futures
from crawlers.saramin_crawler import SaraminCrawler
from crawlers.campuspick_crawler import CampickCrawler
from crawlers.jobkorea_crawler import JobKoreaCrawler
from save_to_firestore import save_job_posting

def run_crawler(crawler):
    """ê° í¬ë¡¤ëŸ¬ë¥¼ ì‹¤í–‰í•˜ê³  í¬ë¡¤ë§ëœ ë°ì´í„° ì €ì¥"""
    print(f"ğŸš€ {crawler.__class__.__name__} í¬ë¡¤ë§ ì‹œì‘...")
    jobs = crawler.crawl_jobs()

    if jobs:
        print(f"ğŸ“‚ Firestore ì €ì¥ ì‹œì‘ (ì´ {len(jobs)}ê°œ ê³µê³ )")
        for job in jobs:
            print(f"ğŸ” í¬ë¡¤ë§ëœ ë°ì´í„°: {job['job']} at {job['company']}")
            save_job_posting(job)
    else:
        print(f"âŒ {crawler.__class__.__name__}ì—ì„œ í¬ë¡¤ë§ëœ ê³µê³  ì—†ìŒ")

if __name__ == "__main__":
    print("âœ¨ ì—¬ëŸ¬ ì±„ìš© ì‚¬ì´íŠ¸ì—ì„œ í¬ë¡¤ë§ì„ ë™ì‹œì— ì‹¤í–‰í•©ë‹ˆë‹¤...")

    # âœ… í¬ë¡¤ëŸ¬ ë¦¬ìŠ¤íŠ¸ (í•„ìš” ì‹œ ì¶”ê°€ ê°€ëŠ¥)
    crawlers = [
        SaraminCrawler(),
        CampickCrawler(),
        JobKoreaCrawler(),
    ]

    # âœ… ë³‘ë ¬ ì²˜ë¦¬í•˜ì—¬ ì—¬ëŸ¬ í¬ë¡¤ëŸ¬ ì‹¤í–‰
    with concurrent.futures.ThreadPoolExecutor() as executor:
        executor.map(run_crawler, crawlers)

    print("âœ… ëª¨ë“  í¬ë¡¤ë§ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
