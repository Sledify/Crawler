from crawlers.base_crawler import BaseCrawler
from datetime import datetime
from save_to_firestore import save_job_posting

class CampickCrawler(BaseCrawler):
    """ ìº í¼ìŠ¤í”½ ì±„ìš© ì •ë³´ë¥¼ í¬ë¡¤ë§í•˜ëŠ” í¬ë¡¤ëŸ¬ """

    def __init__(self):
        super().__init__("https://www.campuspick.com")
        self.job_list_url = f"{self.base_url}/study?category=5"

        # âœ… ìê¸°ì†Œê°œì„œ ë¬¸í•­ ë¦¬ìŠ¤íŠ¸ (ìº í¼ìŠ¤í”½ì— ë§ê²Œ ìˆ˜ì •)
        self.self_intro_questions = [
            "1. ì´ ìŠ¤í„°ë””ì— ì§€ì›í•œ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.",
            "2. ë³¸ì¸ì˜ ì—­í• ê³¼ ê¸°ëŒ€í•˜ëŠ” ì ì„ ì„œìˆ í•˜ì„¸ìš”.",
            "3. íŒ€ì›ë“¤ê³¼ í˜‘ì—… ì‹œ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” ê°€ì¹˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
        ]

    def extract_job_details(self, job_url):
        """ ìƒì„¸ ì±„ìš© ê³µê³ ì—ì„œ ì •ë³´ í¬ë¡¤ë§ """
        soup = self.fetch_page(job_url)
        if not soup:
            return {
                "jobDescription": "í¬ì§€ì…˜ ìƒì„¸ ì—†ìŒ",
                "jobType": "ìŠ¤í„°ë””/ê³µëª¨ì „",
                "preferredQualifications": "ìš°ëŒ€ì‚¬í•­ ì—†ìŒ",
                "qualifications": "ìê²© ìš”ê±´ ì—†ìŒ",
                "deadline": "ë§ˆê°ì¼ ì •ë³´ ì—†ìŒ"
            }

        # âœ… ì œëª© ë° ë§ˆê°ì¼ ì •ë³´
        job_title_element = soup.select_one("article h2")
        job_title = job_title_element.get_text(strip=True) if job_title_element else "ê³µê³  ì œëª© ì—†ìŒ"

        deadline_element = soup.select_one("p.info > span:nth-child(1)")
        deadline = deadline_element.get_text(strip=True) if deadline_element else "ë§ˆê°ì¼ ì •ë³´ ì—†ìŒ"

        # âœ… ê·¼ë¬´ í˜•íƒœ
        job_type = "ìŠ¤í„°ë””/ê³µëª¨ì „"

        # âœ… ê²½ë ¥ ìš”ê±´ (ìº í¼ìŠ¤í”½ì—ëŠ” ë³„ë„ í‘œê¸°ê°€ ì—†ìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ ì„¤ì •)
        qualifications = "ìê²© ìš”ê±´ ì—†ìŒ"
        preferred_qualifications = "ìš°ëŒ€ì‚¬í•­ ì—†ìŒ"

        # âœ… ìƒì„¸ ì„¤ëª…
        job_description_element = soup.select_one("article p.text")
        job_description = job_description_element.get_text(strip=True) if job_description_element else "í¬ì§€ì…˜ ìƒì„¸ ì—†ìŒ"

        return {
            "job": job_title,
            "jobDescription": job_description,
            "jobType": job_type,
            "preferredQualifications": preferred_qualifications,
            "qualifications": qualifications,
            "deadline": deadline
        }

    def crawl_jobs(self):
        """ ìº í¼ìŠ¤í”½ ì±„ìš© ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ì—ì„œ ê³µê³  ìˆ˜ì§‘ """
        print("ğŸ” ìº í¼ìŠ¤í”½ ì±„ìš© ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§ ì‹œì‘...")
        soup = self.fetch_page(self.job_list_url)
        if not soup:
            return []

        job_elements = soup.select("div.list > a")
        job_list = []

        for job in job_elements[:10]:  # ìµœëŒ€ 10ê°œ í¬ë¡¤ë§
            title_element = job.select_one("h2")
            company_element = job.select_one("p.profile")
            link_element = job["href"] if job.has_attr("href") else None

            if title_element and company_element and link_element:
                job_url = f"{self.base_url}{link_element}"

                print(f"ğŸ“Œ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘: {job_url}")

                # âœ… 2ì°¨ í¬ë¡¤ë§ ì¤‘ë‹¨í•˜ê³ , ê¸°ë³¸ ì •ë³´ë§Œ ìˆ˜ì§‘
                job_info = {
                    "site": "Campick",
                    "URL": job_url,
                    "company": company_element.get_text(strip=True),
                    "createdAt": datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC"),
                    "deadline": "ë§ˆê°ì¼ ì •ë³´ ì—†ìŒ",
                    "id": None,
                    "isApplied": False,
                    "job": title_element.get_text(strip=True),
                    "jobDescription": "ìƒì„¸ ì •ë³´ ì—†ìŒ",
                    "jobType": "ìŠ¤í„°ë””/ê³µëª¨ì „",
                    "preferredQualifications": "ìš°ëŒ€ì‚¬í•­ ì—†ìŒ",
                    "qualifications": "ìê²© ìš”ê±´ ì—†ìŒ",
                    "questions": self.self_intro_questions
                }

                job_list.append(job_info)

        return job_list
