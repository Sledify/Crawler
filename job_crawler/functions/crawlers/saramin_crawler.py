from crawlers.base_crawler import BaseCrawler
from datetime import datetime
from save_to_firestore import save_job_posting

class SaraminCrawler(BaseCrawler):
    """ ì‚¬ëŒì¸ ì±„ìš© ì •ë³´ë¥¼ í¬ë¡¤ë§í•˜ëŠ” í¬ë¡¤ëŸ¬ """

    def __init__(self):
        super().__init__("https://www.saramin.co.kr")
        self.job_list_url = f"{self.base_url}/zf_user/jobs/list/job-category?page=1&cat_kewd=84"

        # âœ… ìê¸°ì†Œê°œì„œ ë¬¸í•­ ë¦¬ìŠ¤íŠ¸
        self.self_intro_questions = [
            "1. ì´ ë¶€ë¬¸ì— ì§€ì›í•œ ë™ê¸°ë¥¼ ê¸°ìˆ í•´ ì£¼ì‹­ì‹œì˜¤.",
            "2. ì´ ì§ë¬´ì™€ ê´€ë ¨ëœ ê²½í—˜ì„ ë³¸ì¸ì˜ ì—­í• ê³¼ ì„±ê³¼ ì¤‘ì‹¬ìœ¼ë¡œ ê¸°ìˆ í•´ ì£¼ì‹­ì‹œì˜¤.",
            "3. ë‚˜ì™€ ëŒ€ë‹¤ìˆ˜ì˜ ì˜ê²¬ì´ ë‹¤ë¥¸ ê²½ìš°, ë³¸ì¸ì´ ëŒ€ì²˜í–ˆë˜ ê²½í—˜ê³¼ ê·¸ë¡œë¶€í„° ì–»ì€ êµí›ˆì„ ê¸°ìˆ í•´ ì£¼ì‹­ì‹œì˜¤."
        ]

    def extract_job_details(self, job_url):
        """ ìƒì„¸ ì±„ìš© ê³µê³ ì—ì„œ ì •ë³´ í¬ë¡¤ë§ """
        soup = self.fetch_page(job_url)
        if not soup:
            return {
                "jobDescription": "í¬ì§€ì…˜ ìƒì„¸ ì—†ìŒ",
                "jobType": "ì •ê·œì§",
                "preferredQualifications": "ìš°ëŒ€ì‚¬í•­ ì—†ìŒ",
                "qualifications": "ìê²© ìš”ê±´ ì—†ìŒ",
                "deadline": "ë§ˆê°ì¼ ì •ë³´ ì—†ìŒ"
            }

        card_element = soup.select_one(".card_cont.wrap_recruit_view")
        deadline = card_element["data-closing-date"] if card_element and "data-closing-date" in card_element.attrs else "ë§ˆê°ì¼ ì •ë³´ ì—†ìŒ"
        job_title = card_element["data-recruit-title"] if card_element and "data-recruit-title" in card_element.attrs else "ê³µê³  ì œëª© ì—†ìŒ"

        # âœ… ê·¼ë¬´ í˜•íƒœ (ì •ê·œì§ ë“±)
        job_type_element = soup.select_one(".section_basic_view .wrap_summary_job .list_summary .type")
        job_type = job_type_element.get_text(strip=True) if job_type_element else "ì •ê·œì§"

        # âœ… ê²½ë ¥ ìš”ê±´
        qualifications_element = soup.select_one(".section_basic_view .wrap_summary_job .list_summary .experience")
        qualifications = qualifications_element.get_text(strip=True).replace("\xa0", " ") if qualifications_element else "ìê²© ìš”ê±´ ì—†ìŒ"

        # âœ… í•™ë ¥ ìš”ê±´
        preferred_qualifications_element = soup.select_one(".section_basic_view .wrap_summary_job .list_summary .education")
        preferred_qualifications = preferred_qualifications_element.get_text(strip=True).replace("\xa0", " ") if preferred_qualifications_element else "ìš°ëŒ€ì‚¬í•­ ì—†ìŒ"

        # âœ… ìƒì„¸ ì„¤ëª… í¬ë¡¤ë§ (iframeì´ ìˆëŠ” ê²½ìš°)
        job_description = "í¬ì§€ì…˜ ìƒì„¸ ì—†ìŒ"
        iframe_element = soup.select_one("iframe.recruit_detail_iframe")

        if iframe_element:
            iframe_src = iframe_element.get("data-src") or iframe_element.get("src")
            if iframe_src:
                iframe_url = f"{self.base_url}{iframe_src}"
                iframe_soup = self.fetch_page(iframe_url)
                if iframe_soup:
                    detail_view = iframe_soup.select_one(".cont_detail_view")
                    if detail_view:
                        job_description = detail_view.get_text(strip=True) or "í¬ì§€ì…˜ ìƒì„¸ ì—†ìŒ"

        return {
            "job": job_title,
            "jobDescription": job_description,
            "jobType": job_type,
            "preferredQualifications": preferred_qualifications,
            "qualifications": qualifications,
            "deadline": deadline
        }

    def crawl_jobs(self):
        """ ì‚¬ëŒì¸ ì±„ìš© ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ì—ì„œ ê³µê³  ìˆ˜ì§‘ """
        print("ğŸ” ì‚¬ëŒì¸ ì±„ìš© ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§ ì‹œì‘...")
        soup = self.fetch_page(self.job_list_url)
        if not soup:
            return []

        job_elements = soup.select(".list_item")
        job_list = []

        for job in job_elements[:10]:  # ìµœëŒ€ 10ê°œ í¬ë¡¤ë§
            title_element = job.select_one(".job_tit a")
            company_element = job.select_one(".company_nm a")

            if title_element and company_element:
                relative_link = title_element["href"]

                if "rec_idx=" in relative_link:
                    job_url = f"{self.base_url}/zf_user/jobs/relay/view?rec_idx=" + relative_link.split("rec_idx=")[-1].split("&")[0]

                    print(f"ğŸ“Œ ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§ ì‹œì‘: {job_url}")

                    # âœ… 2ì°¨ í¬ë¡¤ë§ ì¤‘ë‹¨í•˜ê³ , ê¸°ë³¸ ì •ë³´ë§Œ ìˆ˜ì§‘
                    job_info = {
                        "site": "Saramin",
                        "URL": job_url,
                        "company": company_element.get_text(strip=True),
                        "createdAt": datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC"),
                        "deadline": "ë§ˆê°ì¼ ì •ë³´ ì—†ìŒ",
                        "id": None,
                        "isApplied": False,
                        "job": title_element.get_text(strip=True),
                        "jobDescription": "ìƒì„¸ ì •ë³´ ì—†ìŒ",
                        "jobType": "ì •ê·œì§",
                        "preferredQualifications": "ìš°ëŒ€ì‚¬í•­ ì—†ìŒ",
                        "qualifications": "ìê²© ìš”ê±´ ì—†ìŒ",
                        "questions": self.self_intro_questions
                    }

                    job_list.append(job_info)

        return job_list
