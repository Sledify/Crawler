import time
from crawlers.base_crawler import BaseCrawler
from datetime import datetime
from save_to_firestore import save_job_posting

class JobKoreaCrawler(BaseCrawler):
    """ ì¡ì½”ë¦¬ì•„ ì±„ìš© ì •ë³´ë¥¼ í¬ë¡¤ë§í•˜ëŠ” í¬ë¡¤ëŸ¬ """

    def __init__(self):
        super().__init__("https://www.jobkorea.co.kr")
        self.base_search_url = f"{self.base_url}/Search/?stext=ê°œë°œì&tabType=recruit&Page_No=1"
        self.max_pages = 20  # í˜ì´ì§€ íƒìƒ‰ ì œí•œ (ì¶”í›„ ë³€ê²½ ê°€ëŠ¥)

        # âœ… ìê¸°ì†Œê°œì„œ ë¬¸í•­ ë¦¬ìŠ¤íŠ¸
        self.self_intro_questions = [
            "1. ì´ ì§ë¬´ì— ì§€ì›í•œ ë™ê¸°ë¥¼ ì„¤ëª…í•´ ì£¼ì„¸ìš”.",
            "2. ê´€ë ¨ ê²½í—˜ê³¼ ì„±ê³¼ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ê¸°ìˆ í•´ ì£¼ì„¸ìš”.",
            "3. í˜‘ì—… ê²½í—˜ ë° ê°ˆë“± í•´ê²° ë°©ë²•ì„ ì„¤ëª…í•´ ì£¼ì„¸ìš”."
        ]

    def crawl_jobs(self):
        """ ì¡ì½”ë¦¬ì•„ ì±„ìš© ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ì—ì„œ ê³µê³  ìˆ˜ì§‘ """
        print("ğŸ” ì¡ì½”ë¦¬ì•„ ì±„ìš© ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§ ì‹œì‘...")
        job_list = []

        for page_no in range(1, self.max_pages + 1):
            url = f"{self.base_search_url}{page_no}"
            soup = self.fetch_page(url)

            if not soup:
                print(f"âŒ {page_no} í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•¨")
                continue

            job_elements = soup.select("div.list-default > ul > li")

            if not job_elements:
                print(f"âŒ ì¡ì½”ë¦¬ì•„ {page_no} í˜ì´ì§€ì—ì„œ ê³µê³  ì—†ìŒ")
                break  # ë” ì´ìƒ ê³µê³ ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨

            for job in job_elements:
                title_element = job.select_one("a.title")
                company_element = job.select_one("a.name.dev_view")
                exp_element = job.select_one("span.exp")
                edu_element = job.select_one("span.edu")
                loc_element = job.select_one("span.loc.long")
                date_element = job.select_one("span.date")
                keywords_element = job.select_one("p.etc")

                if title_element and company_element:
                    job_url = f"{self.base_url}{title_element['href']}" if title_element.has_attr("href") else "ë§í¬ ì—†ìŒ"

                    job_info = {
                        "site": "JobKorea",
                        "URL": job_url,
                        "company": company_element.get_text(strip=True),
                        "createdAt": datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC"),
                        "deadline": date_element.get_text(strip=True) if date_element else "ë§ˆê°ì¼ ì •ë³´ ì—†ìŒ",
                        "id": None,
                        "isApplied": False,
                        "job": title_element.get_text(strip=True),
                        "jobDescription": "ìƒì„¸ ì •ë³´ ì—†ìŒ",
                        "jobType": exp_element.get_text(strip=True) if exp_element else "ê²½ë ¥ ì •ë³´ ì—†ìŒ",
                        "preferredQualifications": edu_element.get_text(strip=True) if edu_element else "í•™ë ¥ ìš”ê±´ ì—†ìŒ",
                        "qualifications": "ìê²© ìš”ê±´ ì—†ìŒ",
                        "location": loc_element.get_text(strip=True) if loc_element else "ìœ„ì¹˜ ì •ë³´ ì—†ìŒ",
                        "keywords": keywords_element.get_text(strip=True) if keywords_element else "í‚¤ì›Œë“œ ì—†ìŒ",
                        "questions": self.self_intro_questions
                    }

                    job_list.append(job_info)
                    time.sleep(0.1)  # í¬ë¡¤ë§ ì†ë„ ì¡°ì ˆ

            print(f"ğŸ“„ {page_no} í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ (ì´ {len(job_list)}ê°œ ìˆ˜ì§‘)")

        return job_list
